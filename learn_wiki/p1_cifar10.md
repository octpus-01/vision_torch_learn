# ğŸ¯ CIFAR-10 å›¾åƒåˆ†ç±»å®Œæ•´å®æˆ˜æŒ‡å—

## ä¸€ã€é¡¹ç›®æ¦‚è¿°

### æ ¸å¿ƒç›®æ ‡
1. **æ•°æ®åŠ è½½**ï¼šä½¿ç”¨ `torchvision` åŠ è½½ CIFAR-10 æ•°æ®é›†ï¼ˆæœ¬åœ°è·¯å¾„ï¼š`data/cifar10`ï¼‰
2. **æ¨¡å‹æ„å»º**ï¼šå®ç°ä¸‰ç§ä¸åŒå¤æ‚åº¦çš„ CNN æ¨¡å‹ï¼ˆç®€å• CNN / ResNet18 / VGG11ï¼‰
3. **è®­ç»ƒç›‘æ§**ï¼šé›†æˆ TensorBoard å®æ—¶å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
4. **æ€§èƒ½è¯„ä¼°**ï¼šæŒæ¡æ¨¡å‹æ”¶æ•›åˆ¤æ–­ä¸è¿‡æ‹Ÿåˆè¯†åˆ«æ–¹æ³•

### æŠ€æœ¯æ ˆ
- **æ¡†æ¶**ï¼šPyTorch + torchvision
- **å¯è§†åŒ–**ï¼šTensorBoard
- **æ•°æ®é›†**ï¼šCIFAR-10ï¼ˆ32Ã—32 å½©è‰²å›¾åƒï¼Œ10 ä¸ªç±»åˆ«ï¼‰

---

## äºŒã€æ•°æ®åŠ è½½ä¸é¢„å¤„ç†

### 2.1 æ ¸å¿ƒä»£ç å®ç°

```python
import torch
import torchvision
import torchvision.transforms as transforms

# ========== æ•°æ®å¢å¼ºä¸æ ‡å‡†åŒ– ==========
# è®­ç»ƒé›†ï¼šéœ€è¦æ•°æ®å¢å¼ºä»¥æé«˜æ³›åŒ–èƒ½åŠ›
train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),      # éšæœºè£å‰ªï¼Œè¾¹ç¼˜å¡«å……4åƒç´ 
    transforms.RandomHorizontalFlip(p=0.5),    # 50%æ¦‚ç‡æ°´å¹³ç¿»è½¬
    transforms.ToTensor(),                      # è½¬ä¸ºTensorå¹¶å½’ä¸€åŒ–åˆ°[0,1]
    transforms.Normalize(
        mean=(0.4914, 0.4822, 0.4465),         # CIFAR-10è®­ç»ƒé›†å‡å€¼
        std=(0.2470, 0.2435, 0.2616)           # CIFAR-10è®­ç»ƒé›†æ ‡å‡†å·®
    )
])

# æµ‹è¯•é›†ï¼šä»…åšæ ‡å‡†åŒ–ï¼Œä¸åšæ•°æ®å¢å¼º
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(
        mean=(0.4914, 0.4822, 0.4465),
        std=(0.2470, 0.2435, 0.2616)
    )
])

# ========== æ•°æ®é›†åŠ è½½ ==========
# å…³é”®å‚æ•°è¯´æ˜ï¼š
# - root: æŒ‡å‘åŒ…å« cifar-10-batches-py/ å­ç›®å½•çš„çˆ¶ç›®å½•
# - download=False: æ•°æ®å·²å­˜åœ¨ï¼Œç¦æ­¢é‡æ–°ä¸‹è½½
# - train: True=è®­ç»ƒé›†(50000å¼ ), False=æµ‹è¯•é›†(10000å¼ )

trainset = torchvision.datasets.CIFAR10(
    root='data/cifar10',        # ç¡®ä¿è·¯å¾„ä¸‹æœ‰ cifar-10-batches-py/ å­ç›®å½•
    train=True,
    download=False,             # å¿…é¡»è®¾ä¸ºFalseï¼Œé¿å…é‡å¤ä¸‹è½½
    transform=train_transform
)

testset = torchvision.datasets.CIFAR10(
    root='data/cifar10',
    train=False,
    download=False,
    transform=test_transform
)

# ========== DataLoaderé…ç½® ==========
trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size=128,             # å¸¸ç”¨æ‰¹é‡å¤§å°ï¼Œå¯æ ¹æ®æ˜¾å­˜è°ƒæ•´
    shuffle=True,               # æ¯è½®æ‰“ä¹±æ•°æ®é¡ºåº
    num_workers=2,              # å¤šè¿›ç¨‹åŠ è½½ï¼Œå»ºè®®2-4
    pin_memory=True             # åŠ é€ŸGPUæ•°æ®ä¼ è¾“ï¼ˆå¦‚æœä½¿ç”¨CUDAï¼‰
)

testloader = torch.utils.data.DataLoader(
    testset,
    batch_size=128,
    shuffle=False,              # æµ‹è¯•é›†ä¸éœ€è¦æ‰“ä¹±
    num_workers=2
)
```

### 2.2 å…³é”®è¦ç‚¹è§£æ

| å‚æ•°/æ“ä½œ | ä½œç”¨ | æ³¨æ„äº‹é¡¹ |
|-----------|------|----------|
| `RandomCrop(32, padding=4)` | æ•°æ®å¢å¼ºï¼šéšæœºè£å‰ª32Ã—32åŒºåŸŸ | å¿…é¡»åŠ paddingï¼Œå¦åˆ™å›¾åƒå°ºå¯¸ä¼šå˜å° |
| `Normalize(mean, std)` | æ ‡å‡†åŒ–ï¼šä½¿æ•°æ®åˆ†å¸ƒæ¥è¿‘N(0,1) | **å¿…é¡»ä½¿ç”¨CIFAR-10ä¸“ç”¨ç»Ÿè®¡å€¼**ï¼Œä¸èƒ½ç”¨ImageNetçš„ |
| `download=False` | ç¦æ­¢è‡ªåŠ¨ä¸‹è½½ | å¦‚æœè®¾ä¸ºTrueä¸”æ•°æ®å·²å­˜åœ¨ï¼Œä¼šæŠ¥é”™æˆ–é‡å¤ä¸‹è½½ |
| `num_workers` | å¤šè¿›ç¨‹æ•°æ®åŠ è½½ | è®¾ä¸º0åˆ™ä½¿ç”¨ä¸»è¿›ç¨‹ï¼Œè°ƒè¯•æ—¶å»ºè®®è®¾ä¸º0 |
| `root`è·¯å¾„ | æ•°æ®æ ¹ç›®å½• | å¿…é¡»åŒ…å«`cifar-10-batches-py/`å­ç›®å½•ï¼Œç»“æ„å¦‚ä¸‹ï¼š<br>`data/cifar10/cifar-10-batches-py/data_batch_1` |

### 2.3 å¸¸è§ç›®å½•ç»“æ„é”™è¯¯

```
âŒ é”™è¯¯ç»“æ„ï¼ˆç¼ºå°‘å­ç›®å½•ï¼‰ï¼š
data/cifar10/
â”œâ”€â”€ data_batch_1
â”œâ”€â”€ data_batch_2
â””â”€â”€ test_batch

âœ… æ­£ç¡®ç»“æ„ï¼š
data/cifar10/
â””â”€â”€ cifar-10-batches-py/          # å¿…é¡»è¿™å±‚å­ç›®å½•ï¼
    â”œâ”€â”€ data_batch_1
    â”œâ”€â”€ data_batch_2
    â”œâ”€â”€ data_batch_3
    â”œâ”€â”€ data_batch_4
    â”œâ”€â”€ data_batch_5
    â”œâ”€â”€ test_batch
    â””â”€â”€ batches.meta
```

**ä¿®å¤å‘½ä»¤**ï¼š
```bash
mkdir -p data/cifar10/cifar-10-batches-py
mv data/cifar10/data_batch_* data/cifar10/test_batch data/cifar10/batches.meta data/cifar10/cifar-10-batches-py/
```

---

## ä¸‰ã€æ¨¡å‹æ„å»ºæ–¹æ¡ˆï¼ˆä¸‰é€‰ä¸€ï¼‰

### 3.1 æ–¹æ¡ˆ Aï¼šç®€å• CNNï¼ˆé€‚åˆå…¥é—¨ç†è§£ï¼‰

```python
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # ç‰¹å¾æå–å±‚
        self.features = nn.Sequential(
            # è¾“å…¥: 3Ã—32Ã—32 â†’ è¾“å‡º: 32Ã—32Ã—32
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),           # æ‰¹å½’ä¸€åŒ–ï¼ŒåŠ é€Ÿæ”¶æ•›
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),           # 32Ã—32 â†’ 16Ã—16
            
            # è¾“å…¥: 32Ã—16Ã—16 â†’ è¾“å‡º: 64Ã—16Ã—16
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)            # 16Ã—16 â†’ 8Ã—8
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(p=0.5),            # é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(64 * 8 * 8, 128),   # å±•å¹³å: 64é€šé“Ã—8Ã—8ç©ºé—´
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(128, 10)            # 10ä¸ªç±»åˆ«
        )
        
    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)           # ä»ç¬¬1ç»´å¼€å§‹å±•å¹³ï¼Œä¿ç•™batchç»´åº¦
        x = self.classifier(x)
        return x

# ========== å®ä¾‹åŒ–æ¨¡å‹ï¼ˆå…³é”®ï¼å¿…é¡»åŠ æ‹¬å·ï¼‰ ==========
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleCNN().to(device)            # âœ… æ­£ç¡®ï¼šåˆ›å»ºå®ä¾‹åç§»åŠ¨è®¾å¤‡
# model = SimpleCNN.to(device)            # âŒ é”™è¯¯ï¼šå¯¹ç±»è°ƒç”¨.to()
```

**ç»“æ„è§£æ**ï¼š
- è¾“å…¥å°ºå¯¸ï¼š`3Ã—32Ã—32`ï¼ˆ3é€šé“ï¼Œ32Ã—32åƒç´ ï¼‰
- ç»è¿‡2æ¬¡ `MaxPool2d(2,2)`ï¼šç©ºé—´å°ºå¯¸ `32â†’16â†’8`
- æœ€ç»ˆç‰¹å¾å›¾ï¼š`64Ã—8Ã—8 = 4096` ç»´å‘é‡
- å‚æ•°é‡ï¼šçº¦ 20ä¸‡ï¼Œé€‚åˆå¿«é€Ÿå®éªŒ

---

### 3.2 æ–¹æ¡ˆ Bï¼šResNet18ï¼ˆå·¥ä¸šçº§æ ‡å‡†ï¼‰

```python
import torchvision.models as models

def create_resnet18(num_classes=10, pretrained=False):
    """
    åˆ›å»ºé€‚é…CIFAR-10çš„ResNet18
    ä¿®æ”¹ç‚¹ï¼š
    1. é¦–å±‚å·ç§¯ï¼škernel_size 7â†’3, stride 2â†’1, padding 3â†’1
       ï¼ˆCIFAR-10å›¾åƒå¤ªå°ï¼Œä¸éœ€è¦ä¸‹é‡‡æ ·8å€ï¼‰
    2. ç§»é™¤maxpoolï¼ˆæˆ–æ”¹ä¸ºstride=1ï¼‰
    3. å…¨è¿æ¥å±‚è¾“å‡ºæ”¹ä¸º10ç±»
    """
    model = models.resnet18(pretrained=pretrained)
    
    # ä¿®æ”¹é¦–å±‚å·ç§¯é€‚é…32Ã—32è¾“å…¥
    model.conv1 = nn.Conv2d(
        3, 64, kernel_size=3, stride=1, padding=1, bias=False
    )
    model.maxpool = nn.Identity()  # ç§»é™¤æœ€å¤§æ± åŒ–ï¼ˆæˆ–æ”¹ä¸ºnn.MaxPool2d(1,1)ï¼‰
    
    # ä¿®æ”¹è¾“å‡ºå±‚
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    
    return model

# ä½¿ç”¨
model = create_resnet18(num_classes=10, pretrained=False).to(device)
```

**å…³é”®ä¿®æ”¹åŸç†**ï¼š
- åŸå§‹ResNetè®¾è®¡ç”¨äº224Ã—224çš„ImageNet
- CIFAR-10ä»…32Ã—32ï¼Œè‹¥ä¿æŒ`kernel_size=7, stride=2`ï¼Œä¿¡æ¯æŸå¤±è¿‡å¿«
- æ”¹ä¸º`kernel_size=3, stride=1`åï¼Œç‰¹å¾å›¾å°ºå¯¸å˜åŒ–ï¼š`32â†’32â†’16â†’8â†’4`ï¼ˆæ›´åˆç†ï¼‰

---

### 3.3 æ–¹æ¡ˆ Cï¼šVGG11ï¼ˆç»å…¸æ¶æ„ï¼‰

```python
def create_vgg11(num_classes=10, pretrained=False):
    """
    åˆ›å»ºé€‚é…CIFAR-10çš„VGG11
    VGGç‰¹ç‚¹ï¼šè¿ç»­å°å·ç§¯(3Ã—3) + æœ€å¤§æ± åŒ–ï¼Œç»“æ„è§„æ•´
    """
    model = models.vgg11(pretrained=pretrained)
    
    # ä¿®æ”¹é¦–å±‚å·ç§¯ï¼šé€‚é…32Ã—32è¾“å…¥ï¼ˆåŸä¸º224Ã—224ï¼‰
    model.features[0] = nn.Conv2d(3, 64, kernel_size=3, padding=1)
    
    # ä¿®æ”¹åˆ†ç±»å™¨æœ€åä¸€å±‚ï¼š4096 â†’ 10
    model.classifier[6] = nn.Linear(4096, num_classes)
    
    return model

# ä½¿ç”¨
model = create_vgg11(num_classes=10, pretrained=False).to(device)
```

---

## å››ã€TensorBoard è®­ç»ƒç›‘æ§

### 4.1 åŸºç¡€é…ç½®

```python
from torch.utils.tensorboard import SummaryWriter
import time

# åˆ›å»ºwriterï¼ˆå»ºè®®æŒ‰å®éªŒå‘½åï¼‰
experiment_name = f"cifar10_resnet18_{time.strftime('%m%d_%H%M')}"
writer = SummaryWriter(log_dir=f'runs/{experiment_name}')

# å¯é€‰ï¼šè®°å½•æ¨¡å‹ç»“æ„
dummy_input = torch.randn(1, 3, 32, 32).to(device)
writer.add_graph(model, dummy_input)
```

### 4.2 è®­ç»ƒå¾ªç¯ä¸­çš„è®°å½•

```python
def train_epoch(model, loader, criterion, optimizer, epoch, writer, log_interval=100):
    model.train()  # è®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨BatchNorm/Dropoutï¼‰
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (inputs, labels) in enumerate(loader):
        inputs, labels = inputs.to(device), labels.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()           # å¿…é¡»ï¼šæ¸…é›¶æ¢¯åº¦ï¼Œé˜²æ­¢ç´¯åŠ 
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # æ¯log_intervalæ­¥è®°å½•ä¸€æ¬¡ï¼ˆé¿å…æ—¥å¿—è¿‡äºé¢‘ç¹ï¼‰
        global_step = epoch * len(loader) + batch_idx
        if batch_idx % log_interval == 0:
            writer.add_scalar('Train/Loss', loss.item(), global_step)
            writer.add_scalar('Train/Accuracy', 100.*correct/total, global_step)
            
            # è®°å½•å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']
            writer.add_scalar('Train/Learning_Rate', current_lr, global_step)
    
    # è¿”å›epochå¹³å‡æŒ‡æ ‡
    epoch_loss = running_loss / len(loader)
    epoch_acc = 100. * correct / total
    return epoch_loss, epoch_acc
```

### 4.3 è¯„ä¼°ä¸è®°å½•

```python
def evaluate(model, loader, criterion, epoch, writer, tag='Test'):
    model.eval()  # è¯„ä¼°æ¨¡å¼ï¼ˆå†»ç»“BatchNorm/Dropoutï¼‰
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item() * inputs.size(0)  # åŠ æƒå¹³å‡
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    avg_loss = total_loss / total
    accuracy = 100. * correct / total
    
    # è®°å½•åˆ°TensorBoard
    writer.add_scalar(f'{tag}/Loss', avg_loss, epoch)
    writer.add_scalar(f'{tag}/Accuracy', accuracy, epoch)
    
    return avg_loss, accuracy
```

### 4.4 å¯åŠ¨ TensorBoard

```bash
# ç»ˆç«¯è¿è¡Œ
tensorboard --logdir=runs --port=6006

# è®¿é—® http://localhost:6006
# å»ºè®®é¢æ¿å¸ƒå±€ï¼š
# - ä¸Šæ–¹ï¼šTrain/Test Loss å¯¹æ¯”æ›²çº¿
# - ä¸‹æ–¹ï¼šTrain/Test Accuracy å¯¹æ¯”æ›²çº¿
# - å³ä¾§ï¼šLearning Rate å˜åŒ–
```

---

## äº”ã€å¦‚ä½•åˆ¤æ–­æ¨¡å‹æ˜¯å¦è¾¾åˆ°å­¦ä¹ æé™

### 5.1 æ ¸å¿ƒç›‘æ§æŒ‡æ ‡ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰

| æŒ‡æ ‡ | è§‚å¯Ÿé‡ç‚¹ | åˆ¤æ–­æ ‡å‡† |
|------|----------|----------|
| **Test Loss** | æ˜¯å¦æŒç»­ä¸‹é™æˆ–å¼€å§‹ä¸Šå‡ | è¿ç»­5-10è½®ä¸é™ â†’ å¯èƒ½æ”¶æ•›ï¼›<br>å¼€å§‹ä¸Šå‡ â†’ è¿‡æ‹Ÿåˆï¼Œç«‹å³åœæ­¢ |
| **Test Accuracy** | æ³¢åŠ¨èŒƒå›´ä¸è¶‹åŠ¿ | è¿ç»­5è½®å˜åŒ–<0.1% â†’ å¯èƒ½æ”¶æ•› |
| **Train-Test Loss Gap** | å·®è·æ˜¯å¦æŒç»­æ‰©å¤§ | Gap > 1.0 ä¸”æ‰©å¤§ â†’ ä¸¥é‡è¿‡æ‹Ÿåˆ |
| **Gradient Norm** | æ¢¯åº¦èŒƒæ•°æ˜¯å¦è¶‹è¿‘äº0 | < 1e-5 ä¸”ç¨³å®š â†’ åˆ°è¾¾å±€éƒ¨æœ€ä¼˜ |

### 5.2 å…¸å‹æ›²çº¿è§£è¯»

```
ç†æƒ³æƒ…å†µï¼ˆæ­£å¸¸æ”¶æ•›ï¼‰ï¼š
Train Loss:  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  æŒç»­ä¸‹é™
Test Loss:   â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  åŒæ­¥ä¸‹é™ï¼Œæœ€ç»ˆå¹³ç¨³
Test Acc:    â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  å¿«é€Ÿä¸Šå‡åå¹³ç¨³

è¿‡æ‹Ÿåˆï¼ˆéœ€è¦æ­£åˆ™åŒ–/æ—©åœï¼‰ï¼š
Train Loss:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  æ¥è¿‘0
Test Loss:   â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–“â–“â–“  å…ˆé™åå‡ï¼ˆâ–“è¡¨ç¤ºä¸Šå‡ï¼‰
Test Acc:    â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–“â–“â–“  è¾¾åˆ°å³°å€¼åä¸‹é™

æ¬ æ‹Ÿåˆï¼ˆæ¨¡å‹å®¹é‡ä¸è¶³ï¼‰ï¼š
Train Loss:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ä¸‹é™ååœæ»åœ¨è¾ƒé«˜å€¼
Test Loss:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  ä¸Train Lossæ¥è¿‘ä½†éƒ½ä¸ä½
Test Acc:    â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  è¿œä½äºè¯¥æ¨¡å‹åº”æœ‰æ°´å¹³ï¼ˆå¦‚ResNet18<85%ï¼‰
```

### 5.3 å†³ç­–æµç¨‹

```mermaid
graph TD
    A[å¼€å§‹è®­ç»ƒ] --> B{Test Lossæ˜¯å¦ä¸‹é™?}
    B -->|æ˜¯| C[ç»§ç»­è®­ç»ƒ]
    B -->|å¦| D{Test Lossæ˜¯å¦ä¸Šå‡?}
    D -->|æ˜¯| E[è¿‡æ‹Ÿåˆ! åŠ è½½æœ€ä½³æƒé‡åœæ­¢]
    D -->|å¦| F{Test Accè¿ç»­5è½®å˜åŒ–<0.1%?}
    F -->|å¦| C
    F -->|æ˜¯| G{Train-Test Loss Gapæ˜¯å¦ç¨³å®š?}
    G -->|æ˜¯| H[å·²è¾¾å­¦ä¹ æé™]
    G -->|å¦| I[ä¼˜åŒ–é—®é¢˜/å­¦ä¹ ç‡è¿‡å¤§]
    
    C --> J{æ˜¯å¦è¶…è¿‡æœ€å¤§epoch?}
    J -->|å¦| B
    J -->|æ˜¯| H
```

### 5.4 CIFAR-10 æ€§èƒ½åŸºå‡†å‚è€ƒ

| æ¨¡å‹ | é¢„æœŸå‡†ç¡®ç‡ | å…¸å‹æ”¶æ•›epoch | å…³é”®ç“¶é¢ˆ |
|------|------------|---------------|----------|
| SimpleCNN | 70% - 78% | 30-50 | æ¨¡å‹å®¹é‡ä¸è¶³ï¼ˆå‚æ•°é‡å°‘ï¼‰ |
| VGG11 | 88% - 91% | 80-150 | æ¢¯åº¦æ¶ˆå¤±/è®¡ç®—é‡å¤§ |
| ResNet18 | 92% - 94% | 100-200 | éœ€è¦å……åˆ†è®­ç»ƒ |
| ResNet50 | 93% - 95% | 150-300 | å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€å¼ºæ­£åˆ™åŒ– |
| **äººç±»æ°´å¹³** | **~94%** | - | - |
| **å½“å‰SOTA** | **96%+** | - | ä½¿ç”¨å¤æ‚æ¶æ„+æ•°æ®å¢å¼º |

> **åˆ¤æ–­åŸåˆ™**ï¼šå¦‚æœä½ çš„ ResNet18 è®­ç»ƒ 200 epoch åå‡†ç¡®ç‡ < 90%ï¼Œå¤§æ¦‚ç‡æ˜¯ä»£ç /è¶…å‚é—®é¢˜ï¼Œè€Œéæ¨¡å‹æé™ã€‚

---

## å…­ã€å¸¸è§é”™è¯¯ä¸è§£å†³æ–¹æ¡ˆ

### 6.1 è‡´å‘½é”™è¯¯ï¼ˆå¿…ç°æŠ¥é”™ï¼‰

| é”™è¯¯ç°è±¡ | æ ¹æœ¬åŸå›  | è§£å†³æ–¹æ¡ˆ |
|----------|----------|----------|
| `AttributeError: 'torch.device' object has no attribute '_apply'` | å¯¹ç±»è€Œéå®ä¾‹è°ƒç”¨ `.to()` | `model = SimpleCNN().to(device)` åŠ æ‹¬å· |
| `FileNotFoundError: CIFAR10 data not found` | ç›®å½•ç»“æ„é”™è¯¯ | ç¡®ä¿è·¯å¾„ä¸º `root/data/cifar-10-batches-py/` |
| `RuntimeError: CUDA out of memory` | æ˜¾å­˜ä¸è¶³ | å‡å° `batch_size`ï¼ˆå¦‚128â†’64ï¼‰ |
| `ValueError: Expected input batch_size (X) to match target batch_size (Y)` | æ ‡ç­¾ç»´åº¦ä¸åŒ¹é… | æ£€æŸ¥ `criterion(outputs, labels)`ï¼Œlabelsåº”ä¸ºLongTensor |

### 6.2 éšè”½é”™è¯¯ï¼ˆè®­ç»ƒèƒ½è·‘ä½†æ•ˆæœå·®ï¼‰

| ç°è±¡ | åŸå›  | æ£€æŸ¥ç‚¹ |
|------|------|--------|
| è®­ç»ƒlossä¸é™ | å­¦ä¹ ç‡è¿‡å¤§/æ¢¯åº¦æ¶ˆå¤± | å°è¯• `lr=0.0001`ï¼›æ£€æŸ¥BatchNormæ˜¯å¦å¯ç”¨ |
| æµ‹è¯•å‡†ç¡®ç‡å§‹ç»ˆ<60% | å¿˜è®°æ ‡å‡†åŒ–æˆ–æ ‡å‡†åŒ–å‚æ•°é”™è¯¯ | ç¡®è®¤ä½¿ç”¨CIFAR-10ä¸“ç”¨mean/std |
| è®­ç»ƒlosså¾ˆä½ä½†æµ‹è¯•losså¾ˆé«˜ | å¿˜è®° `model.eval()` | è¯„ä¼°å‰å¿…é¡»è°ƒç”¨ï¼ |
| æ¯è½®è®­ç»ƒç»“æœæ³¢åŠ¨æå¤§ | å¿˜è®° `optimizer.zero_grad()` | å¯¼è‡´æ¢¯åº¦ç´¯åŠ  |
| TensorBoardæ— æ•°æ® | è·¯å¾„é”™è¯¯æˆ–æƒé™é—®é¢˜ | æ£€æŸ¥ `log_dir` æ˜¯å¦å­˜åœ¨ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„ |

---

## ä¸ƒã€è¿›é˜¶ä¼˜åŒ–å»ºè®®

### 7.1 è¿›ä¸€æ­¥æå‡æ€§èƒ½

1. **æ•°æ®å¢å¼ºå¢å¼º**ï¼š
   ```python
   transforms.RandomRotation(15),          # éšæœºæ—‹è½¬
   transforms.ColorJitter(brightness=0.2, contrast=0.2),  # é¢œè‰²æŠ–åŠ¨
   transforms.RandomErasing(p=0.5),        # éšæœºæ“¦é™¤ï¼ˆéœ€é…åˆToTensoråï¼‰
   ```

2. **å­¦ä¹ ç‡é¢„çƒ­ï¼ˆWarmupï¼‰**ï¼š
   ```python
   from torch.optim.lr_scheduler import LambdaLR
   
   def warmup_scheduler(epoch):
       if epoch < 5:
           return epoch / 5
       return 1
   
   scheduler = LambdaLR(optimizer, lr_lambda=warmup_scheduler)
   ```

3. **æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰**ï¼š
   ```python
   criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
   ```

4. **æ¨¡å‹é›†æˆï¼ˆEnsembleï¼‰**ï¼š
   - è®­ç»ƒ3ä¸ªä¸åŒåˆå§‹åŒ–çš„ResNet18
   - é¢„æµ‹æ—¶å–å¹³å‡

### 7.2 è°ƒè¯• checklist

- [ ] æ•°æ®åŠ è½½ï¼šæ‰“å° `trainset[0]` ç¡®è®¤å›¾åƒå°ºå¯¸ä¸º32Ã—32ï¼Œæ ‡ç­¾ä¸º0-9æ•´æ•°
- [ ] æ¨¡å‹è¾“å‡ºï¼šç¡®è®¤ `model(torch.randn(2,3,32,32)).shape` ä¸º `torch.Size([2, 10])`
- [ ] è®¾å¤‡ä¸€è‡´æ€§ï¼šç¡®è®¤ `inputs.device == model.device == labels.device`
- [ ] æ¢¯åº¦æ£€æŸ¥ï¼šç¡®è®¤ `model.conv1.weight.grad` éNoneä¸”å€¼åˆç†ï¼ˆéNaN/Infï¼‰

---

## å…«ã€æ€»ç»“

æœ¬æŒ‡å—æ¶µç›–äº†ä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼Œæ ¸å¿ƒè¦ç‚¹ï¼š

1. **æ•°æ®**ï¼šæ­£ç¡®çš„ç›®å½•ç»“æ„ + CIFAR-10ä¸“ç”¨æ ‡å‡†åŒ–å‚æ•°
2. **æ¨¡å‹**ï¼šä¸‰ç§æ–¹æ¡ˆç”±æµ…å…¥æ·±ï¼ŒResNet18æ˜¯æ€§ä»·æ¯”æœ€é«˜çš„é€‰æ‹©
3. **è®­ç»ƒ**ï¼šå¿…é¡»åŒºåˆ† `train()`/`eval()`ï¼Œå®šæœŸè®°å½•TensorBoard
4. **åˆ¤æ–­**ï¼šä»¥ **Test Loss** ä¸ºä¸»è¦æ”¶æ•›æŒ‡æ ‡ï¼Œç»“åˆ Test Accuracy ç»¼åˆåˆ¤æ–­
5. **è°ƒè¯•**ï¼šé‡åˆ°é—®é¢˜æ—¶ï¼Œå…ˆæ£€æŸ¥è®¾å¤‡ã€ç»´åº¦ã€æ¨¡å¼åˆ‡æ¢è¿™ä¸‰è¦ç´ 

> ğŸ’¡ **ä¸‹ä¸€æ­¥å»ºè®®**ï¼šå…ˆç”¨ SimpleCNN è·‘é€šå®Œæ•´æµç¨‹ï¼ˆçº¦30åˆ†é’Ÿï¼‰ï¼Œè§‚å¯ŸTensorBoardæ›²çº¿ç†è§£è®­ç»ƒåŠ¨æ€ï¼Œå†åˆ‡æ¢åˆ° ResNet18 è¿½æ±‚æ›´é«˜ç²¾åº¦ã€‚
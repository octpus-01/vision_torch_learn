# PyTorch 优化器完全指南（高中生版）

## 一、什么是优化器？（用爬山来理解）

想象你在**爬山**，目标是到达山顶（找到最好的模型）。但你蒙着眼睛，只能感受到脚下的坡度。

- **梯度（Gradient）**：告诉你当前位置最陡的方向（是上坡还是下坡）
- **学习率（Learning Rate）**：你每一步迈多大
- **优化器（Optimizer）**：你的"爬山策略"——是直愣愣地走，还是聪明地利用惯性、避开陷阱

> 在机器学习中，我们的目标是最小化**损失函数**（Loss Function），也就是让模型的预测误差越来越小。优化器就是帮我们高效找到最低点的算法。

---

## 二、核心概念速查

| 术语 | 通俗解释 | 类比 |
|------|---------|------|
| **梯度** | 函数在当前点的斜率/变化率 | 山坡的陡峭程度 |
| **学习率 (lr)** | 每次更新的步长大小 | 走路的步幅 |
| **动量 (Momentum)** | 积累之前的运动趋势 | 下坡时的惯性 |
| **自适应学习率** | 根据情况自动调整步长 | 平路大步走，险路小步挪 |
| **权重衰减** | 防止参数变得太大 | 给模型"减肥"，避免过复杂 |

---

## 三、七大经典优化器详解

### 1. SGD（随机梯度下降）— 最基础的"老实人"

```python
torch.optim.SGD(params, lr=0.01, momentum=0, weight_decay=0)
```

**原理**：每次只看一个（或一小批）样本，计算梯度，然后往反方向走一步。

**特点**：
- ✅ 简单、计算快、内存占用小
- ❌ 容易卡在"平缓区"或"局部最小值"，波动大

**通俗理解**：像是一个固执的人，每次只看脚下，一步一步走，容易被小土坡困住。

**带 Momentum 的版本**：
```python
torch.optim.SGD(params, lr=0.01, momentum=0.9)  # 加入惯性
```
> 就像滚雪球下坡，会积累速度，冲过一些小坑，收敛更快。

---

### 2. Momentum（动量法）— 滚雪球效应

**原理**：不仅看当前梯度，还记住之前的运动方向，像物理中的动量。

**公式简化理解**：
```
速度 = 0.9 × 旧速度 + 当前梯度
新位置 = 旧位置 - 学习率 × 速度
```

**通俗理解**：想象滚下山坡的雪球，越滚越快，能冲过平坦区域，也能带着惯性冲出局部小坑。

**优点**：加速收敛，减少震荡

---

### 3. Nesterov Accelerated Gradient (NAG) — 有远见的动量

```python
torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True)
```

**原理**：先"预判"一下动量会带你去哪里，在那个** lookahead 位置**计算梯度，再修正方向。

**通俗理解**：不像普通动量那样盲目冲，而是像赛车手**提前看弯道**，提前减速或转向，更聪明。

**特点**：比标准 Momentum 更稳定，收敛更好

---

### 4. AdaGrad（自适应梯度）— 给每个参数定制步长

```python
torch.optim.Adagrad(params, lr=0.01, weight_decay=0)
```

**原理**：记录每个参数的历史梯度平方和。如果某个参数之前变化很剧烈，就给它**更小的学习率**；如果变化平缓，就给**更大的学习率**。

**通俗理解**：像是一个细心的导游，对常走的路（变化大的参数）让你慢点走仔细看看，对陌生的路（变化小的参数）鼓励你大胆探索。

**特点**：
- ✅ 适合稀疏数据（如自然语言处理）
- ❌ 学习率会越变越小，最后可能"走不动"（过早停止）

---

### 5. RMSprop — 解决 AdaGrad 的"刹车太重"问题

```python
torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08)
```

**原理**：不用全部历史梯度，而是用**指数移动平均**（类似"近期更重要"的思想），避免学习率无限缩小。

**公式核心**：
```
缓存 = 0.9 × 旧缓存 + 0.1 × (当前梯度)²
有效学习率 = 学习率 / √(缓存 + 小常数)
```

**通俗理解**：像开车时看后视镜，但更看重最近的路况，而不是从出发开始的全部历史。

**特点**：
- ✅ 解决了 AdaGrad 学习率单调递减的问题
- ✅ 适合非平稳目标（如 RNN）
- ✅ 是 Hinton 提出的经典方法，非常实用

---

### 6. Adam（自适应矩估计）— 当今最流行的"全能选手"

```python
torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
```

**原理**：**Momentum + RMSprop 的结合体**：
- 第一矩（beta1=0.9）：估计梯度的**均值**（动量思想）
- 第二矩（beta2=0.999）：估计梯度的**方差**（自适应学习率思想）

**通俗理解**：像一个经验丰富的登山者，既带着惯性冲劲（Momentum），又会根据地形自动调整步伐大小（RMSprop）。

**为什么叫"自适应矩估计"**：
- 矩是统计学概念，一阶矩是均值，二阶矩是方差
- Adam 同时利用了这两种统计信息

**特点**：
- ✅ 对大多数问题效果都很好，**默认首选**
- ✅ 对学习率不太敏感（鲁棒性强）
- ✅ 适合大规模数据和高维空间
- ❌ 可能在某些情况下泛化略差（后面会讲）

---

### 7. AdamW — Adam 的改进版

```python
torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)
```

**原理**：修正了 Adam 中 **权重衰减（Weight Decay）的实现方式**。

**关键区别**：

| | Adam | AdamW |
|--|------|-------|
| 权重衰减 | 加在梯度上（与自适应学习率耦合） | 直接加在参数上（解耦） |
| 效果 | L2 正则化效果被削弱 | 真正的权重衰减，正则化效果更好 |

**通俗理解**：Adam 想给模型"减肥"，但减肥计划和学习率调整搅在一起了；AdamW 把减肥计划单独拎出来，效果更好。

**为什么更好**：
- 在 Transformer（如 BERT、GPT）中表现优异
- 权重衰减真正独立，正则化效果更纯粹
- 目前**大模型训练的首选优化器**

---

## 四、优化器对比总结

| 优化器 | 核心思想 | 优点 | 缺点 | 适用场景 |
|--------|---------|------|------|---------|
| **SGD** | 基础梯度下降 | 简单、内存小 | 慢、易卡壳 | 简单问题，或配合 Momentum |
| **SGD+Momentum** | 加惯性 | 加速、减震荡 | 可能冲过头 | 通用，图像分类常用 |
| **NAG** | 预判修正 | 更稳定 | 略复杂 | 需要精细控制时 |
| **AdaGrad** | 累积历史自适应 | 适合稀疏数据 | 学习率过早衰减 | NLP 中的稀疏特征 |
| **RMSprop** | 近期历史自适应 | 解决 AdaGrad 缺陷 | 需要调参 | RNN、非平稳问题 |
| **Adam** | 动量+自适应 | 通用、鲁棒、快 | 可能泛化略差 | **默认首选**，大多数深度学习 |
| **AdamW** | 解耦权重衰减的 Adam | 正则化更好、泛化强 | 略多一个参数 | **Transformer、大模型** |

---

## 五、学习率：优化器的"油门"

### 5.1 学习率的重要性

学习率太大 → 震荡发散，永远找不到最低点  
学习率太小 → 收敛极慢，或卡在局部最小值

### 5.2 学习率调度策略

```python
# 1. 阶梯衰减（Step Decay）
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
# 每 30 个 epoch，学习率乘以 0.1

# 2. 指数衰减
scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)

# 3. 余弦退火（Cosine Annealing）— 热门
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
# 学习率按余弦曲线从初始值降到 0

# 4. 自适应衰减（根据验证集表现）
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)
# 验证集 loss 5 轮不下降，学习率自动降低
```

**通俗理解**：就像开车，一开始可以快点（大学习率），接近目的地时慢慢减速（学习率衰减），避免开过头。

---

## 六、权重衰减（Weight Decay）：模型的"减肥计划"

### 6.1 什么是权重衰减？

在损失函数中加入参数的平方和（L2 正则化）：
```
总损失 = 数据损失 + λ × Σ(参数²)
```

**目的**：让参数不要太大，防止模型过拟合（记住训练数据而不是学习规律）。

### 6.2 为什么 AdamW 比 Adam 好？

**Adam 的问题**：
```python
# Adam 的实现（近似）
梯度 = 真实梯度 + weight_decay × 参数
```
这里的 weight_decay 也被自适应学习率除掉了，效果打折。

**AdamW 的改进**：
```python
# AdamW 的实现
参数 = 参数 - lr × (动量修正后的梯度) - lr × weight_decay × 参数
```
权重衰减直接作用在参数上，不受自适应学习率影响，**正则化效果更纯粹**。

---

## 七、实践建议：如何选择优化器？

### 7.1 快速决策流程

```
开始新项目？
    ↓
首选 AdamW（lr=1e-3 或 5e-4，weight_decay=0.01）
    ↓
如果训练的是 CV 图像模型？
    试试 SGD + Momentum（lr=0.1，可能收敛更好）
    ↓
如果验证集过拟合严重？
    增大 weight_decay，或改用 SGD
    ↓
如果训练 RNN/LSTM？
    试试 RMSprop 或 Adam
    ↓
如果数据非常稀疏（如 NLP 的 one-hot）？
    试试 AdaGrad（但现代多用 Adam）
```

### 7.2 超参数设置经验

| 优化器 | 推荐学习率 | 其他关键参数 |
|--------|-----------|-------------|
| SGD | 0.01 ~ 0.1 | momentum=0.9, nesterov=True |
| Adam | 1e-3 (0.001) | betas=(0.9, 0.999) |
| AdamW | 5e-4 ~ 1e-3 | weight_decay=0.01 或 0.001 |
| RMSprop | 1e-3 | alpha=0.99 |

---

## 八、常见误区与注意事项

### ❌ 误区 1：Adam 总是最好的
**真相**：虽然 Adam 通用，但在图像分类等任务上，精心调参的 SGD+Momentum 往往泛化更好（测试集表现更优）。

### ❌ 误区 2：学习率越小越好
**真相**：太小的学习率会导致训练极慢，且容易卡在局部最小值。适当大的学习率有助于跳出局部最优。

### ❌ 误区 3：忘记 `zero_grad()`
**后果**：梯度会累积，导致参数更新混乱，loss 爆炸或震荡。
```python
optimizer.zero_grad()  # 每次迭代前必须清空梯度！
loss.backward()
optimizer.step()
```

### ❌ 误区 4：混淆 Weight Decay 和 L2 正则化
**真相**：在 Adam 中它们不等价，AdamW 才正确实现了解耦的权重衰减。

---

## 九、总结：
- **SGD** 老实走，一步一步不回头
- **Momentum** 滚雪球，惯性加速冲山头
- **AdaGrad** 细心人，走过的路慢慢行
- **RMSprop** 看近期，忘记遥远的过去
- **Adam** 全能王，动量自适应都在行
- **AdamW** 真王者，减肥计划不掺假

---

## 参考与拓展

- PyTorch 官方文档：https://pytorch.org/docs/stable/optim.html
- 经典论文：Adam (Kingma & Ba, 2015)，AdamW (Loshchilov & Hutter, 2017)
- 可视化推荐：查看优化器在损失曲面上的轨迹（如 https://losslandscape.com）



---
代码解释：
当然可以！下面我将**逐行详细解释**你提供的这段 PyTorch 代码，用清晰、易懂的语言说明每一行的作用，即使你刚开始学深度学习也能看懂。

---

### 🔹 第一部分：导入所有需要的库

```python
import torch
```
> 导入 PyTorch 核心库，用于张量计算和构建神经网络。

```python
import torch.nn as nn
```
> `nn` 是 PyTorch 的神经网络模块，提供如 `Linear`（全连接层）、损失函数等工具。

```python
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
```
> - `TensorDataset`：把输入 `x` 和标签 `y` 打包成一个数据集。
> - `DataLoader`：按批次（batch）加载数据，支持打乱、多进程等。

```python
from torch.optim.sgd import SGD
from torch.optim.adam import Adam
from torch.optim.adagrad import Adagrad
from torch.optim.rmsprop import RMSprop
```
> 导入四种不同的优化器：
> - `SGD`：随机梯度下降（基础版）
> - `Adam`：自适应学习率优化器（最常用）
> - `Adagrad`、`RMSprop`：其他自适应优化算法

> 💡 这些优化器决定了模型如何根据损失来更新参数。

```python
import numpy as np
```
> NumPy 库，用于数值计算（虽然这里主要用 PyTorch，但有时会混用）。

```python
import matplotlib
import matplotlib.pyplot as plt
```
> 用于绘图。`plt` 是画图的主要接口。

---

### 🔹 第二部分：设置中文字体（避免中文显示为方框）

```python
matplotlib.rcParams['font.sans-serif'] = ['Source Han Sans SC']
```
> 设置 Matplotlib 默认使用 **思源黑体（简体中文）** 显示文字。  
> 你的系统已安装该字体（从 `fc-list` 可知），所以中文能正常显示。

```python
matplotlib.rcParams['axes.unicode_minus'] = False
```
> 让坐标轴上的负号（如 `-1`）正确显示，而不是变成方块。

---

### 🔹 第三部分：准备训练数据（目标函数是 y = x³）

```python
x = torch.unsqueeze(torch.linspace(-1, 1, 500), dim=1)
```
> - `torch.linspace(-1, 1, 500)`：在 -1 到 1 之间生成 500 个均匀点（形状 `(500,)`）。
> - `torch.unsqueeze(..., dim=1)`：在第1维增加一个维度，变成 `(500, 1)`，符合神经网络输入格式（每行一个样本）。

```python
y = x.pow(3)
```
> 对每个 `x` 计算立方（`y = x³`），结果 `y` 形状也是 `(500, 1)`。

✅ 现在我们有了 500 个 `(x, x³)` 数据点，用来训练模型。

---

### 🔹 第四部分：设置超参数

```python
LR = 0.01
```
> 学习率（Learning Rate）：控制每次参数更新的步长。太大会震荡，太小会学得慢。

```python
batch_size = 15
```
> 每次训练用 15 个数据点作为一个“批次”（batch）。

```python
epoches = 5
```
> 整个数据集训练 5 轮（每轮遍历全部数据）。

```python
torch.manual_seed(10)
``>
> 设置随机种子为 10，确保每次运行代码时**结果可复现**（比如权重初始化相同）。

---

### 🔹 第五部分：打包数据并创建 DataLoader

```python
dataset = TensorDataset(x, y)
```
> 把 `x` 和 `y` 配对，形成一个数据集对象。

```python
loader = DataLoader(
    dataset=dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2
)
```
> - `batch_size=15`：每次取 15 个样本。
> - `shuffle=True`：每轮训练前打乱数据顺序，防止模型“记住顺序”。
> - `num_workers=2`：用 2 个子进程并行加载数据，加快训练速度。

---

### 🔹 第六部分：定义神经网络类

```python
class Net(nn.Module):
```
> 定义一个叫 `Net` 的神经网络类，继承自 `nn.Module`（PyTorch 标准做法）。

```python
    def __init__(self, n_input, n_hidden, n_output):
        super(Net, self).__init__()
```
> 初始化函数，接收三个参数：
> - `n_input`：输入特征数（这里是 1，因为 x 是标量）
> - `n_hidden`：隐藏层神经元数（这里是 10）
> - `n_output`：输出数（这里是 1，预测 y）

```python
        self.hidden_layer = nn.Linear(n_input, n_hidden)
        self.output_layer = nn.Linear(n_hidden, n_output)
```
> 定义两层：
> - 隐藏层：从 `n_input` 维映射到 `n_hidden` 维
> - 输出层：从 `n_hidden` 维映射到 `n_output` 维

```python
    def forward(self, input):
        x = torch.relu(self.hidden_layer(input))
        output = self.output_layer(x)
        return output
```
> 前向传播过程：
> 1. 输入 → 隐藏层 → ReLU 激活（把负数变0）
> 2. ReLU 输出 → 输出层 → 得到最终预测值
>
> ✅ 注意：**输出层没有激活函数**，因为我们要预测任意实数（x³ 可正可负）。

---

### 🔹 第七部分：`train()` 函数 —— 核心训练逻辑

```python
def train():
```
> 定义一个训练函数，把所有逻辑封装起来。

#### 创建 5 个相同的网络（用于不同优化器）
```python
    net_SGD = Net(1,10,1)
    net_Momentum = Net(1,10,1)
    net_AdaGrad = Net(1,10,1)
    net_RMSprop = Net(1,10,1)
    net_Adam = Net(1,10,1)
    nets = [net_SGD, net_Momentum, net_AdaGrad, net_RMSprop, net_Adam]
```
> 创建 5 个结构完全相同的网络，分别用不同优化器训练，**比较它们的效果**。

#### 为每个网络配置对应的优化器
```python
    optimizer_SGD = SGD(net_SGD.parameters(), lr=LR, momentum=0, weight_decay=0)
```
> - 用 `SGD` 优化器，`momentum=0` 表示纯 SGD（无动量）

```python
    optimizer_Momentum = SGD(net_Momentum.parameters(), lr=LR, momentum=0.9)
```
> - 同样用 `SGD`，但 `momentum=0.9` → 这就是 **带动量的 SGD（Momentum）**

```python
    optimizer_AdaGrad = Adagrad(net_AdaGrad.parameters(), lr=LR, weight_decay=0)
    optimizer_RMSprop = RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
    optimizer_Adam = Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))
```
> 分别配置其他三种优化器，使用推荐的默认参数。

```python
    optimizers = [optimizer_SGD, optimizer_Momentum, optimizer_AdaGrad, optimizer_RMSprop, optimizer_Adam]
```
> 把优化器也打包成列表，方便后面循环使用。

#### 定义损失函数和记录损失的容器
```python
    loss_func = nn.MSELoss()
```
> 使用均方误差（MSE）作为损失函数，衡量预测值和真实值的差距。

```python
    losses = [[],[],[],[],[]]
```
> 创建 5 个空列表，分别记录每个优化器在**每一步**的损失值。

---

### 🔹 第八部分：训练循环

```python
    for epoch in range(epoches):
```
> 总共训练 `epoches=5` 轮。

```python
        for step, (batch_x, batch_y) in enumerate(loader):
```
> 每轮中，遍历所有批次（`batch_x`, `batch_y` 是当前 batch 的输入和标签）。

```python
            for net, optimizer, loss_list in zip(nets, optimizers, losses):
```
> 同时遍历：网络、优化器、损失列表（一一对应）。

```python
                pred_y = net(batch_x)
```
> 用当前网络预测输出。

```python
                loss = loss_func(pred_y, batch_y)
```
> 计算损失。

```python
                optimizer.zero_grad()
```
> **清空上一步的梯度**（非常重要！否则梯度会累加）

```python
                loss.backward()
```
> **反向传播**：自动计算每个参数的梯度。

```python
                optimizer.step()
``>
> **更新参数**：根据梯度和优化器规则调整权重。

```python
                loss_list.append(loss.data.numpy())
```
> 把当前损失值（转成 NumPy 数字）存入对应的列表，用于后续画图。

> 📌 注意：这里**每个 batch 都记录一次损失**，所以横轴是“训练步骤”（不是 epoch）。

---

### 🔹 第九部分：画图比较不同优化器的收敛速度

```python
    plt.figure(figsize=(12,7))
```
> 创建一个 12×7 英寸的大图。

```python
    labels = ['SGD','Momentum','AdaGrad','RMSprop','Adam']
```
> 为每条曲线设置标签。

```python
    for i, loss in enumerate(losses):
        plt.plot(loss, label=labels[i])
```
> 画出每种优化器的损失变化曲线。

```python
    plt.legend(loc='upper right', fontsize=15)
```
> 在右上角显示图例，字号 15。

```python
    plt.tick_params(labelsize=13)
```
> 设置坐标轴刻度数字的大小为 13。

```python
    plt.xlabel("训练步骤", size=15)
    plt.ylabel("模型损失", size=15)
```
> 设置横纵轴标签（中文）。

```python
    plt.ylim((0, 0.3))
```
> 限制 y 轴范围为 0 到 0.3，让曲线更清晰（因为损失下降很快）。

```python
    plt.savefig('optimizerscompare.png')
```
> 把图保存为 `optimizerscompare.png` 文件。

---

### 🔹 最后：启动训练

```python
train()
```
> 调用 `train()` 函数，开始训练和绘图。

---

### ✅ 总结：这段代码在做什么？

> **用同一个简单神经网络（1→10→1）拟合函数 y = x³，分别用 5 种优化器训练，记录每一步的损失，并画图比较它们的收敛速度。**

通常你会发现：
- **Adam 收敛最快**
- **纯 SGD 最慢**
- **带动量的 SGD 比纯 SGD 快**
- **RMSprop / AdaGrad 居中**

这正是深度学习中**优化器选择的重要性**的直观体现！

---

# import everything
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.sgd import SGD
from torch.optim.adam import Adam
from torch.optim.adagrad import Adagrad
from torch.optim.rmsprop import RMSprop
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from torch.utils.tensorboard.writer import SummaryWriter  # â† æ–°å¢ï¼šTensorBoard æ”¯æŒ
import os

# set matplotlib
matplotlib.rcParams['font.sans-serif'] = ['Source Han Sans SC']
matplotlib.rcParams['axes.unicode_minus'] = False

# dataset prepare
x = torch.unsqueeze(torch.linspace(-1, 1, 500), dim=1)
y = x.pow(3)

# set parameters
LR = 0.01
batch_size = 15
epochs = 5
torch.manual_seed(10)

# load data
dataset = TensorDataset(x, y)
loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)

# å…¨å±€æµ‹è¯•æ•°æ®ï¼ˆç”¨äºç”»é¢„æµ‹æ›²çº¿ï¼‰
test_x = x  # shape: (500, 1)
test_y = y  # shape: (500, 1)


class Net(nn.Module):
    def __init__(self, n_input, n_hidden, n_output):
        super(Net, self).__init__()
        self.hidden_layer = nn.Linear(n_input, n_hidden)
        self.output_layer = nn.Linear(n_hidden, n_output)

    def forward(self, input):
        x = torch.relu(self.hidden_layer(input))
        output = self.output_layer(x)
        return output


def train():
    net_SGD = Net(1, 10, 1)
    net_Momentum = Net(1, 10, 1)
    net_AdaGrad = Net(1, 10, 1)
    net_RMSprop = Net(1, 10, 1)
    net_Adam = Net(1, 10, 1)
    nets = [net_SGD, net_Momentum, net_AdaGrad, net_RMSprop, net_Adam]

    # optimizers
    optimizer_SGD = SGD(net_SGD.parameters(), lr=LR, momentum=0, weight_decay=0)
    optimizer_Momentum = SGD(net_Momentum.parameters(), lr=LR, momentum=0.9)
    optimizer_AdaGrad = Adagrad(net_AdaGrad.parameters(), lr=LR, weight_decay=0)
    optimizer_RMSprop = RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
    optimizer_Adam = Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))
    optimizers = [optimizer_SGD, optimizer_Momentum, optimizer_AdaGrad, optimizer_RMSprop, optimizer_Adam]

    loss_func = nn.MSELoss()
    
    # ä¸ºæ¯ä¸ªä¼˜åŒ–å™¨åˆ›å»ºç‹¬ç«‹çš„ TensorBoard writer
    log_dirs = ['runs/SGD', 'runs/Momentum', 'runs/AdaGrad', 'runs/RMSprop', 'runs/Adam']
    writers = [SummaryWriter(log_dir) for log_dir in log_dirs]
    
    # åœ¨ç¬¬ä¸€ä¸ª epoch å¼€å§‹å‰ï¼Œå‘ TensorBoard æ·»åŠ ç½‘ç»œç»“æ„å›¾ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
    dummy_input = torch.randn(1, 1)  # æ¨¡æ‹Ÿè¾“å…¥
    for i, net in enumerate(nets):
        writers[i].add_graph(net, dummy_input)

    step_count = 0  # å…¨å±€ step è®¡æ•°å™¨ï¼ˆç”¨äº TensorBoard æ¨ªè½´ï¼‰

    for epoch in range(epochs):
        print(f"\n========== Epoch {epoch + 1}/{epochs} ==========")
        
        # è®­ç»ƒé˜¶æ®µ
        for step, (batch_x, batch_y) in enumerate(loader):
            for i, (net, optimizer, writer) in enumerate(zip(nets, optimizers, writers)):
                net.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆè™½ç„¶è¿™é‡Œæ²¡ dropout/batchnormï¼Œä½†å¥½ä¹ æƒ¯ï¼‰
                pred_y = net(batch_x)
                loss = loss_func(pred_y, batch_y)
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # è®°å½• loss åˆ° TensorBoard
                writer.add_scalar('Loss/train', loss.item(), step_count)
            
            step_count += 1  # æ¯ä¸ª batch ç®—ä¸€æ­¥

        # ======== æ¯ä¸ª epoch ç»“æŸåï¼šè¯„ä¼°å¹¶ç”»å›¾ ========
        with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ˆèŠ‚çœå†…å­˜ï¼‰
            labels = ['SGD', 'Momentum', 'AdaGrad', 'RMSprop', 'Adam']
            plt.figure(figsize=(12, 7))
            plt.plot(test_x.numpy(), test_y.numpy(), 'r-', label='çœŸå®å‡½æ•° $y=x^3$', linewidth=2)
            
            avg_losses = []
            for i, (net, writer) in enumerate(zip(nets, writers)):
                net.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                pred_test = net(test_x)
                epoch_loss = loss_func(pred_test, test_y).item()
                avg_losses.append(epoch_loss)
                
                # è®°å½• epoch loss åˆ° TensorBoard
                writer.add_scalar('Loss/epoch', epoch_loss, epoch)
                
                # ç”»é¢„æµ‹æ›²çº¿
                plt.plot(test_x.numpy(), pred_test.numpy(), '--', label=f'{labels[i]} (Loss={epoch_loss:.4f})')
            
            plt.legend(fontsize=10)
            plt.xlabel("x", size=12)
            plt.ylabel("y", size=12)
            plt.title(f"Epoch {epoch + 1} - å„ä¼˜åŒ–å™¨æ‹Ÿåˆæ•ˆæœ", size=14)
            plt.grid(True, linestyle='--', alpha=0.6)
            
            # ä¿å­˜å›¾ç‰‡
            os.makedirs('plots', exist_ok=True)
            plt.savefig(f'plots/epoch_{epoch+1:02d}.png', dpi=150, bbox_inches='tight')
            plt.close()  # é‡Šæ”¾å†…å­˜
            
            # æ‰“å°å½“å‰ epoch çš„å¹³å‡æŸå¤±
            print("Epoch Losses:")
            for name, loss_val in zip(labels, avg_losses):
                print(f"  {name}: {loss_val:.6f}")

    # å…³é—­æ‰€æœ‰ writer
    for writer in writers:
        writer.close()

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“Š æŸ¥çœ‹ TensorBoard: åœ¨ç»ˆç«¯è¿è¡Œ â†’ tensorboard --logdir=runs")
    print("ğŸ–¼ï¸ é¢„æµ‹æ›²çº¿å›¾å·²ä¿å­˜åˆ° ./plots/ ç›®å½•")


if __name__ == "__main__":
    train()